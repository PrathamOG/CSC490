{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1RbP5CXUEFLHGM0xzj0f07nKHffjqe3aN",
      "authorship_tag": "ABX9TyOr1sdqxjUzyCnK9LWA6Zw/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U8f9woUpjF9l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_inspect_data(csv_path):\n",
        "    \"\"\"Load CSV (auto-detects delimiter) and perform initial inspection\"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"STEP 1: LOADING AND INSPECTING DATA\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Auto-detect delimiter using csv.Sniffer\n",
        "    import csv\n",
        "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
        "        sample = f.read(2048)\n",
        "        sniffer = csv.Sniffer()\n",
        "        try:\n",
        "            dialect = sniffer.sniff(sample)\n",
        "            sep = dialect.delimiter\n",
        "        except csv.Error:\n",
        "            sep = ','  # fallback if detection fails\n",
        "\n",
        "    print(f\"Detected delimiter: '{sep}'\")\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_csv(csv_path, sep=sep)\n",
        "\n",
        "    # Basic inspection\n",
        "    print(f\"\\nDataset Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
        "    print(f\"\\nColumn Names:\\n{df.columns.tolist()}\")\n",
        "\n",
        "    print(f\"\\nData Types:\")\n",
        "    print(df.dtypes)\n",
        "\n",
        "    print(f\"\\nFirst 3 rows:\")\n",
        "    print(df.head(3))\n",
        "\n",
        "    print(f\"\\nBasic Statistics:\")\n",
        "    print(df.describe())\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "DdFijnL_qiw4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== STEP 2: Handle Missing Values ====================\n",
        "\n",
        "def analyze_missing_values(df):\n",
        "    \"\"\"Analyze missing values in detail\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 2: MISSING VALUES ANALYSIS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    missing = df.isnull().sum()\n",
        "    missing_pct = (missing / len(df)) * 100\n",
        "\n",
        "    missing_df = pd.DataFrame({\n",
        "        'Column': missing.index,\n",
        "        'Missing_Count': missing.values,\n",
        "        'Missing_Percentage': missing_pct.values\n",
        "    })\n",
        "    missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
        "\n",
        "    if len(missing_df) > 0:\n",
        "        print(\"\\nColumns with Missing Values:\")\n",
        "        print(missing_df.to_string(index=False))\n",
        "    else:\n",
        "        print(\"\\nNo missing values found!\")\n",
        "\n",
        "    return missing_df\n",
        "\n",
        "def handle_missing_values(df):\n",
        "    \"\"\"Clean missing values based on column type\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 3: HANDLING MISSING VALUES\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # 1. Remove rows without image URLs (critical for CNN)\n",
        "    before = len(df_clean)\n",
        "    df_clean = df_clean.dropna(subset=['image_url'])\n",
        "    print(f\"\\n1. Removed {before - len(df_clean)} rows without image URLs\")\n",
        "\n",
        "    # 2. Remove rows without product titles (needed for categorization)\n",
        "    before = len(df_clean)\n",
        "    df_clean = df_clean.dropna(subset=['title'])\n",
        "    print(f\"2. Removed {before - len(df_clean)} rows without titles\")\n",
        "\n",
        "    # 3. Handle missing prices - fill with median by brand\n",
        "    if df_clean['price'].isnull().sum() > 0:\n",
        "        print(f\"3. Found {df_clean['price'].isnull().sum()} missing prices\")\n",
        "\n",
        "        # Fill with brand median\n",
        "        df_clean['price'] = df_clean.groupby('brand')['price'].transform(\n",
        "            lambda x: x.fillna(x.median())\n",
        "        )\n",
        "\n",
        "        # If still missing, fill with overall median\n",
        "        df_clean['price'].fillna(df_clean['price'].median(), inplace=True)\n",
        "        print(f\"   Filled missing prices with brand/overall median\")\n",
        "\n",
        "    # 4. Handle missing ratings - fill with brand average\n",
        "    if df_clean['rating'].isnull().sum() > 0:\n",
        "        print(f\"4. Found {df_clean['rating'].isnull().sum()} missing ratings\")\n",
        "\n",
        "        df_clean['rating'] = df_clean.groupby('brand')['rating'].transform(\n",
        "            lambda x: x.fillna(x.mean())\n",
        "        )\n",
        "\n",
        "        # If still missing, fill with overall mean\n",
        "        df_clean['rating'].fillna(df_clean['rating'].mean(), inplace=True)\n",
        "        print(f\"   Filled missing ratings with brand/overall average\")\n",
        "\n",
        "    print(f\"\\nFinal dataset size: {len(df_clean)} rows\")\n",
        "\n",
        "    return df_clean"
      ],
      "metadata": {
        "id": "Web9Cfm96_AT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== STEP 4: Data Validation ====================\n",
        "\n",
        "def validate_data(df):\n",
        "    \"\"\"Validate data quality and constraints\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 4: DATA VALIDATION\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    issues = []\n",
        "\n",
        "    # 1. Check for duplicates\n",
        "    duplicates = df.duplicated(subset=['product_id']).sum()\n",
        "    if duplicates > 0:\n",
        "        issues.append(f\"Found {duplicates} duplicate product_ids\")\n",
        "        print(f\"\\n⚠️  WARNING: {duplicates} duplicate product IDs found\")\n",
        "    else:\n",
        "        print(f\"\\n✓ No duplicate product IDs\")\n",
        "\n",
        "    # 2. Validate URLs\n",
        "    invalid_urls = df[~df['image_url'].str.contains('http', na=False)].shape[0]\n",
        "    if invalid_urls > 0:\n",
        "        issues.append(f\"Found {invalid_urls} invalid image URLs\")\n",
        "        print(f\"⚠️  WARNING: {invalid_urls} invalid image URLs\")\n",
        "    else:\n",
        "        print(f\"✓ All image URLs are valid\")\n",
        "\n",
        "    # 3. Check price range\n",
        "    negative_prices = df[df['price'] < 0].shape[0]\n",
        "    if negative_prices > 0:\n",
        "        issues.append(f\"Found {negative_prices} negative prices\")\n",
        "        print(f\"⚠️  WARNING: {negative_prices} negative prices\")\n",
        "    else:\n",
        "        print(f\"✓ All prices are non-negative\")\n",
        "\n",
        "    # Identify outliers\n",
        "    q1 = df['price'].quantile(0.25)\n",
        "    q3 = df['price'].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    outliers = df[(df['price'] < q1 - 1.5*iqr) | (df['price'] > q3 + 1.5*iqr)].shape[0]\n",
        "    print(f\"   Found {outliers} price outliers (outside 1.5×IQR)\")\n",
        "\n",
        "    # 4. Validate ratings\n",
        "    invalid_ratings = df[(df['rating'] < 0) | (df['rating'] > 5)].shape[0]\n",
        "    if invalid_ratings > 0:\n",
        "        issues.append(f\"Found {invalid_ratings} invalid ratings\")\n",
        "        print(f\"⚠️  WARNING: {invalid_ratings} ratings outside 0-5 range\")\n",
        "    else:\n",
        "        print(f\"✓ All ratings are in valid range (0-5)\")\n",
        "\n",
        "    # 5. Check title length\n",
        "    short_titles = df[df['title'].str.len() < 10].shape[0]\n",
        "    if short_titles > 0:\n",
        "        print(f\"   Found {short_titles} very short titles (<10 chars)\")\n",
        "\n",
        "    return issues\n",
        "\n",
        "def remove_duplicates(df):\n",
        "    \"\"\"Remove duplicate products\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 5: REMOVING DUPLICATES\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    before = len(df)\n",
        "\n",
        "    # Remove exact duplicates\n",
        "    df_clean = df.drop_duplicates(subset=['product_id'], keep='first')\n",
        "\n",
        "    print(f\"\\nRemoved {before - len(df_clean)} duplicate products\")\n",
        "    print(f\"Remaining: {len(df_clean)} unique products\")\n",
        "\n",
        "    return df_clean"
      ],
      "metadata": {
        "id": "re0h9hsWNy6g"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== STEP 6: Category Extraction ====================\n",
        "\n",
        "def extract_categories(df):\n",
        "    \"\"\"Extract product categories from titles\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 6: EXTRACTING PRODUCT CATEGORIES\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Define comprehensive category keywords\n",
        "    category_patterns = {\n",
        "        'Footwear': [\n",
        "            'shoe', 'sneaker', 'boot', 'sandal', 'slipper', 'slide',\n",
        "            'clog', 'flip-flop', 'flip flop', 'footwear', 'trainer',\n",
        "            'loafer', 'moccasin', 'runner', 'running shoe'\n",
        "        ],\n",
        "        'Bags': [\n",
        "            'backpack', 'bag', 'duffel', 'tote', 'handbag', 'satchel',\n",
        "            'crossbody', 'luggage', 'briefcase', 'pouch', 'purse',\n",
        "            'shoulder bag', 'messenger', 'clutch', 'wallet bag'\n",
        "        ],\n",
        "        'Tops': [\n",
        "            'shirt', 't-shirt', 'tee', 'hoodie', 'sweatshirt', 'blouse',\n",
        "            'tank', 'polo', 'sweater', 'jacket', 'coat', 'cardigan',\n",
        "            'blazer', 'vest', 'top'\n",
        "        ],\n",
        "        'Bottoms': [\n",
        "            'pant', 'jean', 'short', 'trouser', 'legging', 'skirt',\n",
        "            'jogger', 'sweatpant', 'cargo'\n",
        "        ],\n",
        "        'Accessories': [\n",
        "            'watch', 'belt', 'hat', 'cap', 'scarf', 'glove',\n",
        "            'sunglasses', 'glasses', 'jewelry', 'jewellery', 'earring',\n",
        "            'necklace', 'bracelet', 'ring', 'umbrella', 'headband',\n",
        "            'tie', 'bowtie', 'suspender'\n",
        "        ],\n",
        "        'Socks': [\n",
        "            'sock', 'hosiery', 'stocking'\n",
        "        ],\n",
        "        'Underwear': [\n",
        "            'underwear', 'brief', 'boxer', 'bra', 'panties', 'lingerie',\n",
        "            'undergarment', 'trunk'\n",
        "        ],\n",
        "        'Sportswear': [\n",
        "            'athletic', 'sport', 'gym', 'fitness', 'training',\n",
        "            'performance', 'active'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    def categorize_product(title):\n",
        "        \"\"\"Categorize a single product based on title\"\"\"\n",
        "        title_lower = title.lower()\n",
        "\n",
        "        # Check each category (order matters for specificity)\n",
        "        for category, keywords in category_patterns.items():\n",
        "            if any(keyword in title_lower for keyword in keywords):\n",
        "                return category\n",
        "\n",
        "        return 'Other'\n",
        "\n",
        "    # Apply categorization\n",
        "    df['product_category'] = df['title'].apply(categorize_product)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\nCategory Distribution:\")\n",
        "    category_counts = df['product_category'].value_counts()\n",
        "\n",
        "    for category, count in category_counts.items():\n",
        "        percentage = (count / len(df)) * 100\n",
        "        print(f\"  {category:15s}: {count:5d} ({percentage:5.2f}%)\")\n",
        "\n",
        "    print(f\"\\nTotal Products: {len(df)}\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "Er1FbNwVpklv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== STEP 7: Text Cleaning ====================\n",
        "\n",
        "def clean_text_fields(df):\n",
        "    \"\"\"Clean and standardize text fields\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 7: CLEANING TEXT FIELDS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # Clean titles\n",
        "    print(\"\\n1. Cleaning product titles...\")\n",
        "    df_clean['title'] = df_clean['title'].str.strip()\n",
        "    df_clean['title'] = df_clean['title'].str.replace(r'\\s+', ' ', regex=True)\n",
        "    df_clean['title'] = df_clean['title'].str.replace(r'[^\\w\\s\\-\\.]', ' ', regex=True)\n",
        "\n",
        "    # Clean brand names\n",
        "    print(\"2. Standardizing brand names...\")\n",
        "    df_clean['brand'] = df_clean['brand'].str.strip()\n",
        "    df_clean['brand'] = df_clean['brand'].str.upper()\n",
        "\n",
        "    # Fix common brand name issues\n",
        "    brand_mapping = {\n",
        "        'JANSPORT': 'JANSPORT',\n",
        "        'JAN SPORT': 'JANSPORT',\n",
        "        'ADIDAS': 'ADIDAS',\n",
        "        'NIKE': 'NIKE',\n",
        "        'PUMA': 'PUMA',\n",
        "        'SKECHERS': 'SKECHERS',\n",
        "        'SKETCHERS': 'SKECHERS',\n",
        "    }\n",
        "    df_clean['brand'] = df_clean['brand'].replace(brand_mapping)\n",
        "\n",
        "    print(f\"   Found {df_clean['brand'].nunique()} unique brands\")\n",
        "\n",
        "    return df_clean"
      ],
      "metadata": {
        "id": "uv_bMDRVpmKe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== STEP 8: Feature Engineering ====================\n",
        "\n",
        "def add_features(df):\n",
        "    \"\"\"Add useful features for analysis\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 8: FEATURE ENGINEERING\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    df_enhanced = df.copy()\n",
        "\n",
        "    # 1. Price bins\n",
        "    df_enhanced['price_category'] = pd.cut(\n",
        "        df_enhanced['price'],\n",
        "        bins=[0, 50, 100, 200, 500, float('inf')],\n",
        "        labels=['Budget', 'Economy', 'Mid-Range', 'Premium', 'Luxury']\n",
        "    )\n",
        "    print(\"\\n1. Added price_category feature\")\n",
        "\n",
        "    # 2. Rating bins\n",
        "    df_enhanced['rating_category'] = pd.cut(\n",
        "        df_enhanced['rating'],\n",
        "        bins=[0, 3.5, 4.0, 4.5, 5.0],\n",
        "        labels=['Low', 'Medium', 'High', 'Excellent']\n",
        "    )\n",
        "    print(\"2. Added rating_category feature\")\n",
        "\n",
        "    # 3. Title length\n",
        "    df_enhanced['title_length'] = df_enhanced['title'].str.len()\n",
        "    print(\"3. Added title_length feature\")\n",
        "\n",
        "    # 4. Word count\n",
        "    df_enhanced['word_count'] = df_enhanced['title'].str.split().str.len()\n",
        "    print(\"4. Added word_count feature\")\n",
        "\n",
        "    # 5. Has discount (if price seems like a discount)\n",
        "    df_enhanced['is_discounted'] = df_enhanced['price'] % 10 == 9\n",
        "    print(\"5. Added is_discounted indicator\")\n",
        "\n",
        "    return df_enhanced"
      ],
      "metadata": {
        "id": "GVv_9JI7AXtv"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== STEP 9: Export Clean Data ====================\n",
        "\n",
        "def export_clean_data(df, output_dir='cleaned_data'):\n",
        "    \"\"\"Export cleaned dataset\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 9: EXPORTING CLEANED DATA\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(exist_ok=True)\n",
        "\n",
        "    # 1. Export full cleaned dataset\n",
        "    csv_path = output_path / 'fashion_products_cleaned.csv'\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\n1. Saved cleaned dataset: {csv_path}\")\n",
        "    print(f\"   Shape: {df.shape}\")\n",
        "\n",
        "    # 2. Export by category\n",
        "    category_dir = output_path / 'by_category'\n",
        "    category_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    for category in df['product_category'].unique():\n",
        "        cat_df = df[df['product_category'] == category]\n",
        "        cat_path = category_dir / f'{category.lower()}_products.csv'\n",
        "        cat_df.to_csv(cat_path, index=False)\n",
        "\n",
        "    print(f\"2. Saved category-specific files to: {category_dir}\")\n",
        "\n",
        "    # 3. Export data summary\n",
        "    summary_path = output_path / 'data_summary.txt'\n",
        "    with open(summary_path, 'w') as f:\n",
        "        f.write(\"FASHION DATASET SUMMARY\\n\")\n",
        "        f.write(\"=\" * 70 + \"\\n\\n\")\n",
        "\n",
        "        f.write(f\"Total Products: {len(df)}\\n\")\n",
        "        f.write(f\"Unique Brands: {df['brand'].nunique()}\\n\")\n",
        "        f.write(f\"Categories: {df['product_category'].nunique()}\\n\\n\")\n",
        "\n",
        "        f.write(\"Category Distribution:\\n\")\n",
        "        f.write(df['product_category'].value_counts().to_string())\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "        f.write(\"Price Statistics:\\n\")\n",
        "        f.write(df['price'].describe().to_string())\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "        f.write(\"Rating Statistics:\\n\")\n",
        "        f.write(df['rating'].describe().to_string())\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "        f.write(\"Top 10 Brands:\\n\")\n",
        "        f.write(df['brand'].value_counts().head(10).to_string())\n",
        "\n",
        "    print(f\"3. Saved data summary: {summary_path}\")\n",
        "\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "212yCjQAAYUr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_preprocessing_pipeline(csv_path):\n",
        "    \"\"\"Run complete preprocessing pipeline\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"FASHION DATASET PREPROCESSING PIPELINE\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Step 1: Load data\n",
        "    df = load_and_inspect_data(csv_path)\n",
        "\n",
        "    # Step 2-3: Handle missing values\n",
        "    missing_analysis = analyze_missing_values(df)\n",
        "    df = handle_missing_values(df)\n",
        "\n",
        "    # Step 4-5: Validate and clean\n",
        "    issues = validate_data(df)\n",
        "    df = remove_duplicates(df)\n",
        "\n",
        "    # Step 6: Extract categories\n",
        "    df = extract_categories(df)\n",
        "\n",
        "    # Step 7: Clean text\n",
        "    df = clean_text_fields(df)\n",
        "\n",
        "     # Step 8: Add features\n",
        "    df = add_features(df)\n",
        "\n",
        "    # Step 9: Export\n",
        "    output_path = export_clean_data(df)\n",
        "\n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"PREPROCESSING COMPLETE!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nFinal Dataset Statistics:\")\n",
        "    print(f\"  Total Products: {len(df)}\")\n",
        "    print(f\"  Categories: {df['product_category'].nunique()}\")\n",
        "    print(f\"  Brands: {df['brand'].nunique()}\")\n",
        "    print(f\"  Price Range: {df['price'].min():.2f} - {df['price'].max():.2f} AED\")\n",
        "    print(f\"  Average Rating: {df['rating'].mean():.2f}\")\n",
        "\n",
        "    print(f\"\\n✓ Cleaned data saved to: {output_path}\")\n",
        "    print(f\"\\nNext Step: Run image download script\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "bcvv3Yk6xUCh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Run the complete pipeline\n",
        "    csv_file = 'products.csv'  # Change to your file path\n",
        "\n",
        "    cleaned_df = run_preprocessing_pipeline(csv_file)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Ready for image download and model training!\")\n",
        "    print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLE7Nbtpv15Q",
        "outputId": "2f6d26e5-2a0c-430d-dbf1-9f65d05d798f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "FASHION DATASET PREPROCESSING PIPELINE\n",
            "======================================================================\n",
            "======================================================================\n",
            "STEP 1: LOADING AND INSPECTING DATA\n",
            "======================================================================\n",
            "Detected delimiter: ','\n",
            "\n",
            "Dataset Shape: 13156 rows × 8 columns\n",
            "\n",
            "Column Names:\n",
            "['product_id', 'brand', 'title', 'price', 'category', 'rating', 'image_url', 'product_url']\n",
            "\n",
            "Data Types:\n",
            "product_id      object\n",
            "brand           object\n",
            "title           object\n",
            "price          float64\n",
            "category        object\n",
            "rating         float64\n",
            "image_url       object\n",
            "product_url     object\n",
            "dtype: object\n",
            "\n",
            "First 3 rows:\n",
            "   product_id     brand                                              title  \\\n",
            "0  B08YRWN3WB  JANSPORT  Big Student Large laptop backpack Black EK0A5B...   \n",
            "1  B08YRXFZZM  JANSPORT                                Superbreak Day Pack   \n",
            "2  B09Q2PQ7ZB   BAODINI  Mini Travel Umbrella With Case Small Compact U...   \n",
            "\n",
            "    price    category  rating  \\\n",
            "0  189.00  New season     4.7   \n",
            "1  119.00  New season     4.6   \n",
            "2   17.79  New season     4.2   \n",
            "\n",
            "                                           image_url  \\\n",
            "0  https://m.media-amazon.com/images/I/51y2EF0OmO...   \n",
            "1  https://m.media-amazon.com/images/I/51yvvQUs3S...   \n",
            "2  https://m.media-amazon.com/images/I/71WbrZPbnG...   \n",
            "\n",
            "                           product_url  \n",
            "0  https://www.amazon.ae/dp/B08YRWN3WB  \n",
            "1  https://www.amazon.ae/dp/B08YRXFZZM  \n",
            "2  https://www.amazon.ae/dp/B09Q2PQ7ZB  \n",
            "\n",
            "Basic Statistics:\n",
            "              price        rating\n",
            "count  12963.000000  12273.000000\n",
            "mean     160.915024      4.235191\n",
            "std      312.435627      0.533392\n",
            "min        0.990000      1.000000\n",
            "25%       45.000000      4.000000\n",
            "50%       93.000000      4.400000\n",
            "75%      188.000000      4.600000\n",
            "max    26000.000000      5.000000\n",
            "\n",
            "======================================================================\n",
            "STEP 2: MISSING VALUES ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "Columns with Missing Values:\n",
            "Column  Missing_Count  Missing_Percentage\n",
            "rating            883            6.711766\n",
            " price            193            1.467011\n",
            " title              1            0.007601\n",
            " brand              1            0.007601\n",
            "\n",
            "======================================================================\n",
            "STEP 3: HANDLING MISSING VALUES\n",
            "======================================================================\n",
            "\n",
            "1. Removed 0 rows without image URLs\n",
            "2. Removed 1 rows without titles\n",
            "3. Found 193 missing prices\n",
            "   Filled missing prices with brand/overall median\n",
            "4. Found 883 missing ratings\n",
            "   Filled missing ratings with brand/overall average\n",
            "\n",
            "Final dataset size: 13155 rows\n",
            "\n",
            "======================================================================\n",
            "STEP 4: DATA VALIDATION\n",
            "======================================================================\n",
            "\n",
            "⚠️  WARNING: 1787 duplicate product IDs found\n",
            "✓ All image URLs are valid\n",
            "✓ All prices are non-negative\n",
            "   Found 1013 price outliers (outside 1.5×IQR)\n",
            "✓ All ratings are in valid range (0-5)\n",
            "   Found 90 very short titles (<10 chars)\n",
            "\n",
            "======================================================================\n",
            "STEP 5: REMOVING DUPLICATES\n",
            "======================================================================\n",
            "\n",
            "Removed 1787 duplicate products\n",
            "Remaining: 11368 unique products\n",
            "\n",
            "======================================================================\n",
            "STEP 6: EXTRACTING PRODUCT CATEGORIES\n",
            "======================================================================\n",
            "\n",
            "Category Distribution:\n",
            "  Bags           :  2405 (21.16%)\n",
            "  Footwear       :  2214 (19.48%)\n",
            "  Accessories    :  2013 (17.71%)\n",
            "  Tops           :  1875 (16.49%)\n",
            "  Other          :  1715 (15.09%)\n",
            "  Bottoms        :   589 ( 5.18%)\n",
            "  Socks          :   297 ( 2.61%)\n",
            "  Underwear      :   159 ( 1.40%)\n",
            "  Sportswear     :   101 ( 0.89%)\n",
            "\n",
            "Total Products: 11368\n",
            "\n",
            "======================================================================\n",
            "STEP 7: CLEANING TEXT FIELDS\n",
            "======================================================================\n",
            "\n",
            "1. Cleaning product titles...\n",
            "2. Standardizing brand names...\n",
            "   Found 2380 unique brands\n",
            "\n",
            "======================================================================\n",
            "STEP 8: FEATURE ENGINEERING\n",
            "======================================================================\n",
            "\n",
            "1. Added price_category feature\n",
            "2. Added rating_category feature\n",
            "3. Added title_length feature\n",
            "4. Added word_count feature\n",
            "5. Added is_discounted indicator\n",
            "\n",
            "======================================================================\n",
            "STEP 9: EXPORTING CLEANED DATA\n",
            "======================================================================\n",
            "\n",
            "1. Saved cleaned dataset: cleaned_data/fashion_products_cleaned.csv\n",
            "   Shape: (11368, 14)\n",
            "2. Saved category-specific files to: cleaned_data/by_category\n",
            "3. Saved data summary: cleaned_data/data_summary.txt\n",
            "\n",
            "======================================================================\n",
            "PREPROCESSING COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "Final Dataset Statistics:\n",
            "  Total Products: 11368\n",
            "  Categories: 9\n",
            "  Brands: 2380\n",
            "  Price Range: 0.99 - 26000.00 AED\n",
            "  Average Rating: 4.21\n",
            "\n",
            "✓ Cleaned data saved to: cleaned_data\n",
            "\n",
            "Next Step: Run image download script\n",
            "\n",
            "======================================================================\n",
            "Ready for image download and model training!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# ==================== 1. CATEGORY DISTRIBUTION ====================\n",
        "\n",
        "def plot_category_distribution(df, save_path='plots'):\n",
        "    \"\"\"Visualize category distribution\"\"\"\n",
        "    Path(save_path).mkdir(exist_ok=True)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Bar chart\n",
        "    category_counts = df['product_category'].value_counts()\n",
        "    axes[0].barh(category_counts.index, category_counts.values, color='steelblue')\n",
        "    axes[0].set_xlabel('Number of Products', fontsize=12, fontweight='bold')\n",
        "    axes[0].set_title('Product Count by Category', fontsize=14, fontweight='bold')\n",
        "    axes[0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "    # Add count labels\n",
        "    for i, v in enumerate(category_counts.values):\n",
        "        axes[0].text(v + 50, i, str(v), va='center', fontweight='bold')\n",
        "\n",
        "    # Pie chart\n",
        "    colors = plt.cm.Set3(range(len(category_counts)))\n",
        "    wedges, texts, autotexts = axes[1].pie(\n",
        "        category_counts.values,\n",
        "        labels=category_counts.index,\n",
        "        autopct='%1.1f%%',\n",
        "        colors=colors,\n",
        "        startangle=90\n",
        "    )\n",
        "    axes[1].set_title('Category Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "    for autotext in autotexts:\n",
        "        autotext.set_color('white')\n",
        "        autotext.set_fontweight('bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_path}/category_distribution.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# ==================== 2. CLASS IMBALANCE ====================\n",
        "\n",
        "def plot_class_balance(df, save_path='plots'):\n",
        "    \"\"\"Check for class imbalance issues\"\"\"\n",
        "    Path(save_path).mkdir(exist_ok=True)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    category_counts = df['product_category'].value_counts()\n",
        "\n",
        "    # 1. Class balance visualization\n",
        "    axes[0].bar(range(len(category_counts)), category_counts.values, color='steelblue')\n",
        "    axes[0].set_xticks(range(len(category_counts)))\n",
        "    axes[0].set_xticklabels(category_counts.index, rotation=45, ha='right')\n",
        "    axes[0].set_ylabel('Number of Samples', fontsize=11, fontweight='bold')\n",
        "    axes[0].set_title('Class Balance Check', fontsize=13, fontweight='bold')\n",
        "    axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add threshold line for minimum recommended samples\n",
        "    min_recommended = 300\n",
        "    axes[0].axhline(y=min_recommended, color='r', linestyle='--',\n",
        "                    linewidth=2, label=f'Min Recommended: {min_recommended}')\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Add value labels\n",
        "    for i, v in enumerate(category_counts.values):\n",
        "        axes[0].text(i, v + 50, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "    # 2. Imbalance ratio\n",
        "    max_samples = category_counts.max()\n",
        "    imbalance_ratios = max_samples / category_counts\n",
        "\n",
        "    colors = ['green' if ratio < 2 else 'orange' if ratio < 5 else 'red'\n",
        "              for ratio in imbalance_ratios]\n",
        "\n",
        "    axes[1].barh(category_counts.index, imbalance_ratios.values, color=colors)\n",
        "    axes[1].set_xlabel('Imbalance Ratio (Max/Current)', fontsize=11, fontweight='bold')\n",
        "    axes[1].set_title('Class Imbalance Analysis', fontsize=13, fontweight='bold')\n",
        "    axes[1].axvline(x=2, color='orange', linestyle='--', linewidth=1, alpha=0.5, label='Moderate (2x)')\n",
        "    axes[1].axvline(x=5, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Severe (5x)')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(axis='x', alpha=0.3)\n",
        "    axes[1].invert_yaxis()\n",
        "\n",
        "    # Add value labels\n",
        "    for i, v in enumerate(imbalance_ratios.values):\n",
        "        axes[1].text(v + 0.1, i, f'{v:.2f}x', va='center', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_path}/class_balance.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Print warnings\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Class Balance Report:\")\n",
        "    print(\"-\" * 70)\n",
        "    severe_imbalance = imbalance_ratios[imbalance_ratios > 5]\n",
        "    if len(severe_imbalance) > 0:\n",
        "        print(\"Categories with severe imbalance (>5x):\")\n",
        "        for cat in severe_imbalance.index:\n",
        "            print(f\"  - {cat}: {category_counts[cat]} samples ({imbalance_ratios[cat]:.2f}x)\")\n",
        "    else:\n",
        "        print(\"✓ Class balance is acceptable (all classes within 5x of largest)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "# ==================== 3. CORRELATION HEATMAP ====================\n",
        "\n",
        "def plot_correlation_heatmap(df, save_path='plots'):\n",
        "    \"\"\"Plot correlation heatmap for numerical features\"\"\"\n",
        "    Path(save_path).mkdir(exist_ok=True)\n",
        "\n",
        "    # Select numerical columns\n",
        "    numeric_cols = ['price', 'rating', 'title_length', 'word_count']\n",
        "    numeric_cols = [col for col in numeric_cols if col in df.columns]\n",
        "\n",
        "    if len(numeric_cols) < 2:\n",
        "        print(\"Not enough numerical features for correlation analysis\")\n",
        "        return\n",
        "\n",
        "    corr_matrix = df[numeric_cols].corr()\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "                square=True, linewidths=1, fmt='.3f',\n",
        "                cbar_kws={\"shrink\": 0.8})\n",
        "    plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_path}/correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# ==================== MAIN EXECUTION ====================\n",
        "\n",
        "def run_essential_visualizations(csv_path, plot_dir='plots'):\n",
        "    # Load cleaned data\n",
        "    print(\"Loading dataset...\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(f\"✓ Loaded {len(df):,} records\")\n",
        "    print(f\"✓ Found {df['product_category'].nunique()} categories\\n\")\n",
        "\n",
        "    # Generate visualizations\n",
        "    print(\"Generating visualizations...\\n\")\n",
        "\n",
        "    plot_category_distribution(df, plot_dir)\n",
        "    plot_class_balance(df, plot_dir)\n",
        "    plot_correlation_heatmap(df, plot_dir)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"VISUALIZATION COMPLETE!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nAll plots saved to: {plot_dir}/\")\n",
        "    print(\"  - category_distribution.png\")\n",
        "    print(\"  - class_balance.png\")\n",
        "    print(\"  - correlation_heatmap.png\")\n",
        "\n",
        "# ==================== USAGE ====================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run on cleaned data\n",
        "    csv_file = 'cleaned_data/fashion_products_cleaned.csv'\n",
        "\n",
        "    run_essential_visualizations(csv_file, plot_dir='plots')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt7NUJusXsLP",
        "outputId": "4fea2720-6e09-4b61-e51d-9a030c9de4f3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset...\n",
            "✓ Loaded 11,368 records\n",
            "✓ Found 9 categories\n",
            "\n",
            "Generating visualizations...\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Class Balance Report:\n",
            "----------------------------------------------------------------------\n",
            "Categories with severe imbalance (>5x):\n",
            "  - Socks: 297 samples (8.10x)\n",
            "  - Underwear: 159 samples (15.13x)\n",
            "  - Sportswear: 101 samples (23.81x)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "VISUALIZATION COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "All plots saved to: plots/\n",
            "  - category_distribution.png\n",
            "  - class_balance.png\n",
            "  - correlation_heatmap.png\n"
          ]
        }
      ]
    }
  ]
}