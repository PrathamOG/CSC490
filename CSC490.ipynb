{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNC5382yBO8A3tgmJ/54kCC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U8f9woUpjF9l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_inspect_data(csv_path):\n",
        "    \"\"\"Load CSV (auto-detects delimiter) and perform initial inspection\"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"STEP 1: LOADING AND INSPECTING DATA\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Auto-detect delimiter using csv.Sniffer\n",
        "    import csv\n",
        "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
        "        sample = f.read(2048)\n",
        "        sniffer = csv.Sniffer()\n",
        "        try:\n",
        "            dialect = sniffer.sniff(sample)\n",
        "            sep = dialect.delimiter\n",
        "        except csv.Error:\n",
        "            sep = ','  # fallback if detection fails\n",
        "\n",
        "    print(f\"Detected delimiter: '{sep}'\")\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_csv(csv_path, sep=sep)\n",
        "\n",
        "    # Basic inspection\n",
        "    print(f\"\\nDataset Shape: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
        "    print(f\"\\nColumn Names:\\n{df.columns.tolist()}\")\n",
        "\n",
        "    print(f\"\\nData Types:\")\n",
        "    print(df.dtypes)\n",
        "\n",
        "    print(f\"\\nFirst 3 rows:\")\n",
        "    print(df.head(3))\n",
        "\n",
        "    print(f\"\\nBasic Statistics:\")\n",
        "    print(df.describe())\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "DdFijnL_qiw4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== STEP 2: Handle Missing Values ====================\n",
        "\n",
        "def analyze_missing_values(df):\n",
        "    \"\"\"Analyze missing values in detail\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 2: MISSING VALUES ANALYSIS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    missing = df.isnull().sum()\n",
        "    missing_pct = (missing / len(df)) * 100\n",
        "\n",
        "    missing_df = pd.DataFrame({\n",
        "        'Column': missing.index,\n",
        "        'Missing_Count': missing.values,\n",
        "        'Missing_Percentage': missing_pct.values\n",
        "    })\n",
        "    missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
        "\n",
        "    if len(missing_df) > 0:\n",
        "        print(\"\\nColumns with Missing Values:\")\n",
        "        print(missing_df.to_string(index=False))\n",
        "    else:\n",
        "        print(\"\\nNo missing values found!\")\n",
        "\n",
        "    return missing_df\n",
        "\n",
        "def handle_missing_values(df):\n",
        "    \"\"\"Clean missing values based on column type\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 3: HANDLING MISSING VALUES\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # 1. Remove rows without image URLs (critical for CNN)\n",
        "    before = len(df_clean)\n",
        "    df_clean = df_clean.dropna(subset=['image_url'])\n",
        "    print(f\"\\n1. Removed {before - len(df_clean)} rows without image URLs\")\n",
        "\n",
        "    # 2. Remove rows without product titles (needed for categorization)\n",
        "    before = len(df_clean)\n",
        "    df_clean = df_clean.dropna(subset=['title'])\n",
        "    print(f\"2. Removed {before - len(df_clean)} rows without titles\")\n",
        "\n",
        "    # 3. Handle missing prices - fill with median by brand\n",
        "    if df_clean['price'].isnull().sum() > 0:\n",
        "        print(f\"3. Found {df_clean['price'].isnull().sum()} missing prices\")\n",
        "\n",
        "        # Fill with brand median\n",
        "        df_clean['price'] = df_clean.groupby('brand')['price'].transform(\n",
        "            lambda x: x.fillna(x.median())\n",
        "        )\n",
        "\n",
        "        # If still missing, fill with overall median\n",
        "        df_clean['price'].fillna(df_clean['price'].median(), inplace=True)\n",
        "        print(f\"   Filled missing prices with brand/overall median\")\n",
        "\n",
        "    # 4. Handle missing ratings - fill with brand average\n",
        "    if df_clean['rating'].isnull().sum() > 0:\n",
        "        print(f\"4. Found {df_clean['rating'].isnull().sum()} missing ratings\")\n",
        "\n",
        "        df_clean['rating'] = df_clean.groupby('brand')['rating'].transform(\n",
        "            lambda x: x.fillna(x.mean())\n",
        "        )\n",
        "\n",
        "        # If still missing, fill with overall mean\n",
        "        df_clean['rating'].fillna(df_clean['rating'].mean(), inplace=True)\n",
        "        print(f\"   Filled missing ratings with brand/overall average\")\n",
        "\n",
        "    print(f\"\\nFinal dataset size: {len(df_clean)} rows\")\n",
        "\n",
        "    return df_clean"
      ],
      "metadata": {
        "id": "Web9Cfm96_AT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== STEP 4: Data Validation ====================\n",
        "\n",
        "def validate_data(df):\n",
        "    \"\"\"Validate data quality and constraints\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 4: DATA VALIDATION\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    issues = []\n",
        "\n",
        "    # 1. Check for duplicates\n",
        "    duplicates = df.duplicated(subset=['product_id']).sum()\n",
        "    if duplicates > 0:\n",
        "        issues.append(f\"Found {duplicates} duplicate product_ids\")\n",
        "        print(f\"\\n⚠️  WARNING: {duplicates} duplicate product IDs found\")\n",
        "    else:\n",
        "        print(f\"\\n✓ No duplicate product IDs\")\n",
        "\n",
        "    # 2. Validate URLs\n",
        "    invalid_urls = df[~df['image_url'].str.contains('http', na=False)].shape[0]\n",
        "    if invalid_urls > 0:\n",
        "        issues.append(f\"Found {invalid_urls} invalid image URLs\")\n",
        "        print(f\"⚠️  WARNING: {invalid_urls} invalid image URLs\")\n",
        "    else:\n",
        "        print(f\"✓ All image URLs are valid\")\n",
        "\n",
        "    # 3. Check price range\n",
        "    negative_prices = df[df['price'] < 0].shape[0]\n",
        "    if negative_prices > 0:\n",
        "        issues.append(f\"Found {negative_prices} negative prices\")\n",
        "        print(f\"⚠️  WARNING: {negative_prices} negative prices\")\n",
        "    else:\n",
        "        print(f\"✓ All prices are non-negative\")\n",
        "\n",
        "    # Identify outliers\n",
        "    q1 = df['price'].quantile(0.25)\n",
        "    q3 = df['price'].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    outliers = df[(df['price'] < q1 - 1.5*iqr) | (df['price'] > q3 + 1.5*iqr)].shape[0]\n",
        "    print(f\"   Found {outliers} price outliers (outside 1.5×IQR)\")\n",
        "\n",
        "    # 4. Validate ratings\n",
        "    invalid_ratings = df[(df['rating'] < 0) | (df['rating'] > 5)].shape[0]\n",
        "    if invalid_ratings > 0:\n",
        "        issues.append(f\"Found {invalid_ratings} invalid ratings\")\n",
        "        print(f\"⚠️  WARNING: {invalid_ratings} ratings outside 0-5 range\")\n",
        "    else:\n",
        "        print(f\"✓ All ratings are in valid range (0-5)\")\n",
        "\n",
        "    # 5. Check title length\n",
        "    short_titles = df[df['title'].str.len() < 10].shape[0]\n",
        "    if short_titles > 0:\n",
        "        print(f\"   Found {short_titles} very short titles (<10 chars)\")\n",
        "\n",
        "    return issues\n",
        "\n",
        "def remove_duplicates(df):\n",
        "    \"\"\"Remove duplicate products\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 5: REMOVING DUPLICATES\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    before = len(df)\n",
        "\n",
        "    # Remove exact duplicates\n",
        "    df_clean = df.drop_duplicates(subset=['product_id'], keep='first')\n",
        "\n",
        "    print(f\"\\nRemoved {before - len(df_clean)} duplicate products\")\n",
        "    print(f\"Remaining: {len(df_clean)} unique products\")\n",
        "\n",
        "    return df_clean"
      ],
      "metadata": {
        "id": "re0h9hsWNy6g"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_preprocessing_pipeline(csv_path):\n",
        "    \"\"\"Run complete preprocessing pipeline\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"FASHION DATASET PREPROCESSING PIPELINE\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Step 1: Load data\n",
        "    df = load_and_inspect_data(csv_path)\n",
        "\n",
        "    # Step 2-3: Handle missing values\n",
        "    missing_analysis = analyze_missing_values(df)\n",
        "    df = handle_missing_values(df)\n",
        "\n",
        "    # Step 4-5: Validate and clean\n",
        "    issues = validate_data(df)\n",
        "    df = remove_duplicates(df)"
      ],
      "metadata": {
        "id": "bcvv3Yk6xUCh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Run the complete pipeline\n",
        "    csv_file = 'products.csv'  # Change to your file path\n",
        "\n",
        "    cleaned_df = run_preprocessing_pipeline(csv_file)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Ready for image download and model training!\")\n",
        "    print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLE7Nbtpv15Q",
        "outputId": "590ce63e-47a6-4b79-cbf3-a6f248bcfa39"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "FASHION DATASET PREPROCESSING PIPELINE\n",
            "======================================================================\n",
            "======================================================================\n",
            "STEP 1: LOADING AND INSPECTING DATA\n",
            "======================================================================\n",
            "Detected delimiter: ','\n",
            "\n",
            "Dataset Shape: 13156 rows × 8 columns\n",
            "\n",
            "Column Names:\n",
            "['product_id', 'brand', 'title', 'price', 'category', 'rating', 'image_url', 'product_url']\n",
            "\n",
            "Data Types:\n",
            "product_id      object\n",
            "brand           object\n",
            "title           object\n",
            "price          float64\n",
            "category        object\n",
            "rating         float64\n",
            "image_url       object\n",
            "product_url     object\n",
            "dtype: object\n",
            "\n",
            "First 3 rows:\n",
            "   product_id     brand                                              title  \\\n",
            "0  B08YRWN3WB  JANSPORT  Big Student Large laptop backpack Black EK0A5B...   \n",
            "1  B08YRXFZZM  JANSPORT                                Superbreak Day Pack   \n",
            "2  B09Q2PQ7ZB   BAODINI  Mini Travel Umbrella With Case Small Compact U...   \n",
            "\n",
            "    price    category  rating  \\\n",
            "0  189.00  New season     4.7   \n",
            "1  119.00  New season     4.6   \n",
            "2   17.79  New season     4.2   \n",
            "\n",
            "                                           image_url  \\\n",
            "0  https://m.media-amazon.com/images/I/51y2EF0OmO...   \n",
            "1  https://m.media-amazon.com/images/I/51yvvQUs3S...   \n",
            "2  https://m.media-amazon.com/images/I/71WbrZPbnG...   \n",
            "\n",
            "                           product_url  \n",
            "0  https://www.amazon.ae/dp/B08YRWN3WB  \n",
            "1  https://www.amazon.ae/dp/B08YRXFZZM  \n",
            "2  https://www.amazon.ae/dp/B09Q2PQ7ZB  \n",
            "\n",
            "Basic Statistics:\n",
            "              price        rating\n",
            "count  12963.000000  12273.000000\n",
            "mean     160.915024      4.235191\n",
            "std      312.435627      0.533392\n",
            "min        0.990000      1.000000\n",
            "25%       45.000000      4.000000\n",
            "50%       93.000000      4.400000\n",
            "75%      188.000000      4.600000\n",
            "max    26000.000000      5.000000\n",
            "\n",
            "======================================================================\n",
            "STEP 2: MISSING VALUES ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "Columns with Missing Values:\n",
            "Column  Missing_Count  Missing_Percentage\n",
            "rating            883            6.711766\n",
            " price            193            1.467011\n",
            " title              1            0.007601\n",
            " brand              1            0.007601\n",
            "\n",
            "======================================================================\n",
            "STEP 3: HANDLING MISSING VALUES\n",
            "======================================================================\n",
            "\n",
            "1. Removed 0 rows without image URLs\n",
            "2. Removed 1 rows without titles\n",
            "3. Found 193 missing prices\n",
            "   Filled missing prices with brand/overall median\n",
            "4. Found 883 missing ratings\n",
            "   Filled missing ratings with brand/overall average\n",
            "\n",
            "Final dataset size: 13155 rows\n",
            "\n",
            "======================================================================\n",
            "STEP 4: DATA VALIDATION\n",
            "======================================================================\n",
            "\n",
            "⚠️  WARNING: 1787 duplicate product IDs found\n",
            "✓ All image URLs are valid\n",
            "✓ All prices are non-negative\n",
            "   Found 1013 price outliers (outside 1.5×IQR)\n",
            "✓ All ratings are in valid range (0-5)\n",
            "   Found 90 very short titles (<10 chars)\n",
            "\n",
            "======================================================================\n",
            "STEP 5: REMOVING DUPLICATES\n",
            "======================================================================\n",
            "\n",
            "Removed 1787 duplicate products\n",
            "Remaining: 11368 unique products\n",
            "\n",
            "======================================================================\n",
            "Ready for image download and model training!\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}