{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8f9woUpjF9l"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "et5utvMO9cQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdFijnL_qiw4"
      },
      "outputs": [],
      "source": [
        "def load_and_inspect_data(csv_path):\n",
        "    \"\"\"Load CSV (auto-detects delimiter) and perform initial inspection\"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"STEP 1: LOADING AND INSPECTING DATA\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Auto-detect delimiter using csv.Sniffer\n",
        "    import csv\n",
        "    with open(csv_path, 'r', encoding='utf-8') as f:\n",
        "        sample = f.read(2048)\n",
        "        sniffer = csv.Sniffer()\n",
        "        try:\n",
        "            dialect = sniffer.sniff(sample)\n",
        "            sep = dialect.delimiter\n",
        "        except csv.Error:\n",
        "            sep = ','  # fallback if detection fails\n",
        "\n",
        "    print(f\"Detected delimiter: '{sep}'\")\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_csv(csv_path, sep=sep)\n",
        "\n",
        "    # Basic inspection\n",
        "    print(f\"\\nDataset Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
        "    print(f\"\\nColumn Names:\\n{df.columns.tolist()}\")\n",
        "\n",
        "    print(f\"\\nData Types:\")\n",
        "    print(df.dtypes)\n",
        "\n",
        "    print(f\"\\nFirst 3 rows:\")\n",
        "    print(df.head(3))\n",
        "\n",
        "    print(f\"\\nBasic Statistics:\")\n",
        "    print(df.describe())\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Web9Cfm96_AT"
      },
      "outputs": [],
      "source": [
        "# ==================== STEP 2: Handle Missing Values ====================\n",
        "\n",
        "def analyze_missing_values(df):\n",
        "    \"\"\"Analyze missing values in detail\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 2: MISSING VALUES ANALYSIS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    missing = df.isnull().sum()\n",
        "    missing_pct = (missing / len(df)) * 100\n",
        "\n",
        "    missing_df = pd.DataFrame({\n",
        "        'Column': missing.index,\n",
        "        'Missing_Count': missing.values,\n",
        "        'Missing_Percentage': missing_pct.values\n",
        "    })\n",
        "    missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
        "\n",
        "    if len(missing_df) > 0:\n",
        "        print(\"\\nColumns with Missing Values:\")\n",
        "        print(missing_df.to_string(index=False))\n",
        "    else:\n",
        "        print(\"\\nNo missing values found!\")\n",
        "\n",
        "    return missing_df\n",
        "\n",
        "def handle_missing_values(df):\n",
        "    \"\"\"Clean missing values based on column type\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 3: HANDLING MISSING VALUES\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # 1. Remove rows without image URLs (critical for CNN)\n",
        "    before = len(df_clean)\n",
        "    df_clean = df_clean.dropna(subset=['image_url'])\n",
        "    print(f\"\\n1. Removed {before - len(df_clean)} rows without image URLs\")\n",
        "\n",
        "    # 2. Remove rows without product titles (needed for categorization)\n",
        "    before = len(df_clean)\n",
        "    df_clean = df_clean.dropna(subset=['title'])\n",
        "    print(f\"2. Removed {before - len(df_clean)} rows without titles\")\n",
        "\n",
        "    # 3. Handle missing prices - fill with median by brand\n",
        "    if df_clean['price'].isnull().sum() > 0:\n",
        "        print(f\"3. Found {df_clean['price'].isnull().sum()} missing prices\")\n",
        "\n",
        "        # Fill with brand median\n",
        "        df_clean['price'] = df_clean.groupby('brand')['price'].transform(\n",
        "            lambda x: x.fillna(x.median())\n",
        "        )\n",
        "\n",
        "        # If still missing, fill with overall median\n",
        "        df_clean['price'].fillna(df_clean['price'].median(), inplace=True)\n",
        "        print(f\"   Filled missing prices with brand/overall median\")\n",
        "\n",
        "    # 4. Handle missing ratings - fill with brand average\n",
        "    if df_clean['rating'].isnull().sum() > 0:\n",
        "        print(f\"4. Found {df_clean['rating'].isnull().sum()} missing ratings\")\n",
        "\n",
        "        df_clean['rating'] = df_clean.groupby('brand')['rating'].transform(\n",
        "            lambda x: x.fillna(x.mean())\n",
        "        )\n",
        "\n",
        "        # If still missing, fill with overall mean\n",
        "        df_clean['rating'].fillna(df_clean['rating'].mean(), inplace=True)\n",
        "        print(f\"   Filled missing ratings with brand/overall average\")\n",
        "\n",
        "    print(f\"\\nFinal dataset size: {len(df_clean)} rows\")\n",
        "\n",
        "    return df_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re0h9hsWNy6g"
      },
      "outputs": [],
      "source": [
        "# ==================== STEP 4: Data Validation ====================\n",
        "\n",
        "def validate_data(df):\n",
        "    \"\"\"Validate data quality and constraints\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 4: DATA VALIDATION\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    issues = []\n",
        "\n",
        "    # 1. Check for duplicates\n",
        "    duplicates = df.duplicated(subset=['product_id']).sum()\n",
        "    if duplicates > 0:\n",
        "        issues.append(f\"Found {duplicates} duplicate product_ids\")\n",
        "        print(f\"\\n‚ö†Ô∏è  WARNING: {duplicates} duplicate product IDs found\")\n",
        "    else:\n",
        "        print(f\"\\n‚úì No duplicate product IDs\")\n",
        "\n",
        "    # 2. Validate URLs\n",
        "    invalid_urls = df[~df['image_url'].str.contains('http', na=False)].shape[0]\n",
        "    if invalid_urls > 0:\n",
        "        issues.append(f\"Found {invalid_urls} invalid image URLs\")\n",
        "        print(f\"‚ö†Ô∏è  WARNING: {invalid_urls} invalid image URLs\")\n",
        "    else:\n",
        "        print(f\"‚úì All image URLs are valid\")\n",
        "\n",
        "    # 3. Check price range\n",
        "    negative_prices = df[df['price'] < 0].shape[0]\n",
        "    if negative_prices > 0:\n",
        "        issues.append(f\"Found {negative_prices} negative prices\")\n",
        "        print(f\"‚ö†Ô∏è  WARNING: {negative_prices} negative prices\")\n",
        "    else:\n",
        "        print(f\"‚úì All prices are non-negative\")\n",
        "\n",
        "    # Identify outliers\n",
        "    q1 = df['price'].quantile(0.25)\n",
        "    q3 = df['price'].quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    outliers = df[(df['price'] < q1 - 1.5*iqr) | (df['price'] > q3 + 1.5*iqr)].shape[0]\n",
        "    print(f\"   Found {outliers} price outliers (outside 1.5√óIQR)\")\n",
        "\n",
        "    # 4. Validate ratings\n",
        "    invalid_ratings = df[(df['rating'] < 0) | (df['rating'] > 5)].shape[0]\n",
        "    if invalid_ratings > 0:\n",
        "        issues.append(f\"Found {invalid_ratings} invalid ratings\")\n",
        "        print(f\"‚ö†Ô∏è  WARNING: {invalid_ratings} ratings outside 0-5 range\")\n",
        "    else:\n",
        "        print(f\"‚úì All ratings are in valid range (0-5)\")\n",
        "\n",
        "    # 5. Check title length\n",
        "    short_titles = df[df['title'].str.len() < 10].shape[0]\n",
        "    if short_titles > 0:\n",
        "        print(f\"   Found {short_titles} very short titles (<10 chars)\")\n",
        "\n",
        "    return issues\n",
        "\n",
        "def remove_duplicates(df):\n",
        "    \"\"\"Remove duplicate products\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 5: REMOVING DUPLICATES\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    before = len(df)\n",
        "\n",
        "    # Remove exact duplicates\n",
        "    df_clean = df.drop_duplicates(subset=['product_id'], keep='first')\n",
        "\n",
        "    print(f\"\\nRemoved {before - len(df_clean)} duplicate products\")\n",
        "    print(f\"Remaining: {len(df_clean)} unique products\")\n",
        "\n",
        "    return df_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er1FbNwVpklv"
      },
      "outputs": [],
      "source": [
        "# ==================== STEP 6: Category Extraction ====================\n",
        "\n",
        "def extract_categories(df):\n",
        "    \"\"\"Extract product categories from titles\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 6: EXTRACTING PRODUCT CATEGORIES\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Define comprehensive category keywords\n",
        "    category_patterns = {\n",
        "        'Footwear': [\n",
        "            'shoe', 'sneaker', 'boot', 'sandal', 'slipper', 'slide',\n",
        "            'clog', 'flip-flop', 'flip flop', 'footwear', 'trainer',\n",
        "            'loafer', 'moccasin', 'runner', 'running shoe'\n",
        "        ],\n",
        "        'Bags': [\n",
        "            'backpack', 'bag', 'duffel', 'tote', 'handbag', 'satchel',\n",
        "            'crossbody', 'luggage', 'briefcase', 'pouch', 'purse',\n",
        "            'shoulder bag', 'messenger', 'clutch', 'wallet bag'\n",
        "        ],\n",
        "        'Tops': [\n",
        "            'shirt', 't-shirt', 'tee', 'hoodie', 'sweatshirt', 'blouse',\n",
        "            'tank', 'polo', 'sweater', 'jacket', 'coat', 'cardigan',\n",
        "            'blazer', 'vest', 'top'\n",
        "        ],\n",
        "        'Bottoms': [\n",
        "            'pant', 'jean', 'short', 'trouser', 'legging', 'skirt',\n",
        "            'jogger', 'sweatpant', 'cargo'\n",
        "        ],\n",
        "        'Accessories': [\n",
        "            'watch', 'belt', 'hat', 'cap', 'scarf', 'glove',\n",
        "            'sunglasses', 'glasses', 'jewelry', 'jewellery', 'earring',\n",
        "            'necklace', 'bracelet', 'ring', 'umbrella', 'headband',\n",
        "            'tie', 'bowtie', 'suspender'\n",
        "        ],\n",
        "        'Socks': [\n",
        "            'sock', 'hosiery', 'stocking'\n",
        "        ],\n",
        "        'Underwear': [\n",
        "            'underwear', 'brief', 'boxer', 'bra', 'panties', 'lingerie',\n",
        "            'undergarment', 'trunk'\n",
        "        ],\n",
        "        'Sportswear': [\n",
        "            'athletic', 'sport', 'gym', 'fitness', 'training',\n",
        "            'performance', 'active'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    def categorize_product(title):\n",
        "        \"\"\"Categorize a single product based on title\"\"\"\n",
        "        title_lower = title.lower()\n",
        "\n",
        "        # Check each category (order matters for specificity)\n",
        "        for category, keywords in category_patterns.items():\n",
        "            if any(keyword in title_lower for keyword in keywords):\n",
        "                return category\n",
        "\n",
        "        return 'Other'\n",
        "\n",
        "    # Apply categorization\n",
        "    df['product_category'] = df['title'].apply(categorize_product)\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\nCategory Distribution:\")\n",
        "    category_counts = df['product_category'].value_counts()\n",
        "\n",
        "    for category, count in category_counts.items():\n",
        "        percentage = (count / len(df)) * 100\n",
        "        print(f\"  {category:15s}: {count:5d} ({percentage:5.2f}%)\")\n",
        "\n",
        "    print(f\"\\nTotal Products: {len(df)}\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uv_bMDRVpmKe"
      },
      "outputs": [],
      "source": [
        "# ==================== STEP 7: Text Cleaning ====================\n",
        "\n",
        "def clean_text_fields(df):\n",
        "    \"\"\"Clean and standardize text fields\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 7: CLEANING TEXT FIELDS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    # Clean titles\n",
        "    print(\"\\n1. Cleaning product titles...\")\n",
        "    df_clean['title'] = df_clean['title'].str.strip()\n",
        "    df_clean['title'] = df_clean['title'].str.replace(r'\\s+', ' ', regex=True)\n",
        "    df_clean['title'] = df_clean['title'].str.replace(r'[^\\w\\s\\-\\.]', ' ', regex=True)\n",
        "\n",
        "    # Clean brand names\n",
        "    print(\"2. Standardizing brand names...\")\n",
        "    df_clean['brand'] = df_clean['brand'].str.strip()\n",
        "    df_clean['brand'] = df_clean['brand'].str.upper()\n",
        "\n",
        "    # Fix common brand name issues\n",
        "    brand_mapping = {\n",
        "        'JANSPORT': 'JANSPORT',\n",
        "        'JAN SPORT': 'JANSPORT',\n",
        "        'ADIDAS': 'ADIDAS',\n",
        "        'NIKE': 'NIKE',\n",
        "        'PUMA': 'PUMA',\n",
        "        'SKECHERS': 'SKECHERS',\n",
        "        'SKETCHERS': 'SKECHERS',\n",
        "    }\n",
        "    df_clean['brand'] = df_clean['brand'].replace(brand_mapping)\n",
        "\n",
        "    print(f\"   Found {df_clean['brand'].nunique()} unique brands\")\n",
        "\n",
        "    return df_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVv_9JI7AXtv"
      },
      "outputs": [],
      "source": [
        "# ==================== STEP 8: Feature Engineering ====================\n",
        "\n",
        "def add_features(df):\n",
        "    \"\"\"Add useful features for analysis\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 8: FEATURE ENGINEERING\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    df_enhanced = df.copy()\n",
        "\n",
        "    # 1. Price bins\n",
        "    df_enhanced['price_category'] = pd.cut(\n",
        "        df_enhanced['price'],\n",
        "        bins=[0, 50, 100, 200, 500, float('inf')],\n",
        "        labels=['Budget', 'Economy', 'Mid-Range', 'Premium', 'Luxury']\n",
        "    )\n",
        "    print(\"\\n1. Added price_category feature\")\n",
        "\n",
        "    # 2. Rating bins\n",
        "    df_enhanced['rating_category'] = pd.cut(\n",
        "        df_enhanced['rating'],\n",
        "        bins=[0, 3.5, 4.0, 4.5, 5.0],\n",
        "        labels=['Low', 'Medium', 'High', 'Excellent']\n",
        "    )\n",
        "    print(\"2. Added rating_category feature\")\n",
        "\n",
        "    # 3. Title length\n",
        "    df_enhanced['title_length'] = df_enhanced['title'].str.len()\n",
        "    print(\"3. Added title_length feature\")\n",
        "\n",
        "    # 4. Word count\n",
        "    df_enhanced['word_count'] = df_enhanced['title'].str.split().str.len()\n",
        "    print(\"4. Added word_count feature\")\n",
        "\n",
        "    # 5. Has discount (if price seems like a discount)\n",
        "    df_enhanced['is_discounted'] = df_enhanced['price'] % 10 == 9\n",
        "    print(\"5. Added is_discounted indicator\")\n",
        "\n",
        "    return df_enhanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "212yCjQAAYUr"
      },
      "outputs": [],
      "source": [
        "# ==================== STEP 9: Export Clean Data ====================\n",
        "\n",
        "def export_clean_data(df, output_dir='cleaned_data'):\n",
        "    \"\"\"Export cleaned dataset\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"STEP 9: EXPORTING CLEANED DATA\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(exist_ok=True)\n",
        "\n",
        "    # 1. Export full cleaned dataset\n",
        "    csv_path = output_path / 'fashion_products_cleaned.csv'\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"\\n1. Saved cleaned dataset: {csv_path}\")\n",
        "    print(f\"   Shape: {df.shape}\")\n",
        "\n",
        "    # 2. Export by category\n",
        "    category_dir = output_path / 'by_category'\n",
        "    category_dir.mkdir(exist_ok=True)\n",
        "\n",
        "    for category in df['product_category'].unique():\n",
        "        cat_df = df[df['product_category'] == category]\n",
        "        cat_path = category_dir / f'{category.lower()}_products.csv'\n",
        "        cat_df.to_csv(cat_path, index=False)\n",
        "\n",
        "    print(f\"2. Saved category-specific files to: {category_dir}\")\n",
        "\n",
        "    # 3. Export data summary\n",
        "    summary_path = output_path / 'data_summary.txt'\n",
        "    with open(summary_path, 'w') as f:\n",
        "        f.write(\"FASHION DATASET SUMMARY\\n\")\n",
        "        f.write(\"=\" * 70 + \"\\n\\n\")\n",
        "\n",
        "        f.write(f\"Total Products: {len(df)}\\n\")\n",
        "        f.write(f\"Unique Brands: {df['brand'].nunique()}\\n\")\n",
        "        f.write(f\"Categories: {df['product_category'].nunique()}\\n\\n\")\n",
        "\n",
        "        f.write(\"Category Distribution:\\n\")\n",
        "        f.write(df['product_category'].value_counts().to_string())\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "        f.write(\"Price Statistics:\\n\")\n",
        "        f.write(df['price'].describe().to_string())\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "        f.write(\"Rating Statistics:\\n\")\n",
        "        f.write(df['rating'].describe().to_string())\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "        f.write(\"Top 10 Brands:\\n\")\n",
        "        f.write(df['brand'].value_counts().head(10).to_string())\n",
        "\n",
        "    print(f\"3. Saved data summary: {summary_path}\")\n",
        "\n",
        "    return output_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcvv3Yk6xUCh"
      },
      "outputs": [],
      "source": [
        "def run_preprocessing_pipeline(csv_path):\n",
        "    \"\"\"Run complete preprocessing pipeline\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"FASHION DATASET PREPROCESSING PIPELINE\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Step 1: Load data\n",
        "    df = load_and_inspect_data(csv_path)\n",
        "\n",
        "    # Step 2-3: Handle missing values\n",
        "    missing_analysis = analyze_missing_values(df)\n",
        "    df = handle_missing_values(df)\n",
        "\n",
        "    # Step 4-5: Validate and clean\n",
        "    issues = validate_data(df)\n",
        "    df = remove_duplicates(df)\n",
        "\n",
        "    # Step 6: Extract categories\n",
        "    df = extract_categories(df)\n",
        "\n",
        "    # Step 7: Clean text\n",
        "    df = clean_text_fields(df)\n",
        "\n",
        "     # Step 8: Add features\n",
        "    df = add_features(df)\n",
        "\n",
        "    # Step 9: Export\n",
        "    output_path = export_clean_data(df)\n",
        "\n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"PREPROCESSING COMPLETE!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nFinal Dataset Statistics:\")\n",
        "    print(f\"  Total Products: {len(df)}\")\n",
        "    print(f\"  Categories: {df['product_category'].nunique()}\")\n",
        "    print(f\"  Brands: {df['brand'].nunique()}\")\n",
        "    print(f\"  Price Range: {df['price'].min():.2f} - {df['price'].max():.2f} AED\")\n",
        "    print(f\"  Average Rating: {df['rating'].mean():.2f}\")\n",
        "\n",
        "    print(f\"\\n‚úì Cleaned data saved to: {output_path}\")\n",
        "    print(f\"\\nNext Step: Run image download script\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLE7Nbtpv15Q",
        "outputId": "2665b7c3-078e-46a3-b239-70f04d87c60f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "FASHION DATASET PREPROCESSING PIPELINE\n",
            "======================================================================\n",
            "======================================================================\n",
            "STEP 1: LOADING AND INSPECTING DATA\n",
            "======================================================================\n",
            "Detected delimiter: ','\n",
            "\n",
            "Dataset Shape: 13156 rows √ó 8 columns\n",
            "\n",
            "Column Names:\n",
            "['product_id', 'brand', 'title', 'price', 'category', 'rating', 'image_url', 'product_url']\n",
            "\n",
            "Data Types:\n",
            "product_id      object\n",
            "brand           object\n",
            "title           object\n",
            "price          float64\n",
            "category        object\n",
            "rating         float64\n",
            "image_url       object\n",
            "product_url     object\n",
            "dtype: object\n",
            "\n",
            "First 3 rows:\n",
            "   product_id     brand                                              title  \\\n",
            "0  B08YRWN3WB  JANSPORT  Big Student Large laptop backpack Black EK0A5B...   \n",
            "1  B08YRXFZZM  JANSPORT                                Superbreak Day Pack   \n",
            "2  B09Q2PQ7ZB   BAODINI  Mini Travel Umbrella With Case Small Compact U...   \n",
            "\n",
            "    price    category  rating  \\\n",
            "0  189.00  New season     4.7   \n",
            "1  119.00  New season     4.6   \n",
            "2   17.79  New season     4.2   \n",
            "\n",
            "                                           image_url  \\\n",
            "0  https://m.media-amazon.com/images/I/51y2EF0OmO...   \n",
            "1  https://m.media-amazon.com/images/I/51yvvQUs3S...   \n",
            "2  https://m.media-amazon.com/images/I/71WbrZPbnG...   \n",
            "\n",
            "                           product_url  \n",
            "0  https://www.amazon.ae/dp/B08YRWN3WB  \n",
            "1  https://www.amazon.ae/dp/B08YRXFZZM  \n",
            "2  https://www.amazon.ae/dp/B09Q2PQ7ZB  \n",
            "\n",
            "Basic Statistics:\n",
            "              price        rating\n",
            "count  12963.000000  12273.000000\n",
            "mean     160.915024      4.235191\n",
            "std      312.435627      0.533392\n",
            "min        0.990000      1.000000\n",
            "25%       45.000000      4.000000\n",
            "50%       93.000000      4.400000\n",
            "75%      188.000000      4.600000\n",
            "max    26000.000000      5.000000\n",
            "\n",
            "======================================================================\n",
            "STEP 2: MISSING VALUES ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "Columns with Missing Values:\n",
            "Column  Missing_Count  Missing_Percentage\n",
            "rating            883            6.711766\n",
            " price            193            1.467011\n",
            " title              1            0.007601\n",
            " brand              1            0.007601\n",
            "\n",
            "======================================================================\n",
            "STEP 3: HANDLING MISSING VALUES\n",
            "======================================================================\n",
            "\n",
            "1. Removed 0 rows without image URLs\n",
            "2. Removed 1 rows without titles\n",
            "3. Found 193 missing prices\n",
            "   Filled missing prices with brand/overall median\n",
            "4. Found 883 missing ratings\n",
            "   Filled missing ratings with brand/overall average\n",
            "\n",
            "Final dataset size: 13155 rows\n",
            "\n",
            "======================================================================\n",
            "STEP 4: DATA VALIDATION\n",
            "======================================================================\n",
            "\n",
            "‚ö†Ô∏è  WARNING: 1787 duplicate product IDs found\n",
            "‚úì All image URLs are valid\n",
            "‚úì All prices are non-negative\n",
            "   Found 1013 price outliers (outside 1.5√óIQR)\n",
            "‚úì All ratings are in valid range (0-5)\n",
            "   Found 90 very short titles (<10 chars)\n",
            "\n",
            "======================================================================\n",
            "STEP 5: REMOVING DUPLICATES\n",
            "======================================================================\n",
            "\n",
            "Removed 1787 duplicate products\n",
            "Remaining: 11368 unique products\n",
            "\n",
            "======================================================================\n",
            "STEP 6: EXTRACTING PRODUCT CATEGORIES\n",
            "======================================================================\n",
            "\n",
            "Category Distribution:\n",
            "  Bags           :  2405 (21.16%)\n",
            "  Footwear       :  2214 (19.48%)\n",
            "  Accessories    :  2013 (17.71%)\n",
            "  Tops           :  1875 (16.49%)\n",
            "  Other          :  1715 (15.09%)\n",
            "  Bottoms        :   589 ( 5.18%)\n",
            "  Socks          :   297 ( 2.61%)\n",
            "  Underwear      :   159 ( 1.40%)\n",
            "  Sportswear     :   101 ( 0.89%)\n",
            "\n",
            "Total Products: 11368\n",
            "\n",
            "======================================================================\n",
            "STEP 7: CLEANING TEXT FIELDS\n",
            "======================================================================\n",
            "\n",
            "1. Cleaning product titles...\n",
            "2. Standardizing brand names...\n",
            "   Found 2380 unique brands\n",
            "\n",
            "======================================================================\n",
            "STEP 8: FEATURE ENGINEERING\n",
            "======================================================================\n",
            "\n",
            "1. Added price_category feature\n",
            "2. Added rating_category feature\n",
            "3. Added title_length feature\n",
            "4. Added word_count feature\n",
            "5. Added is_discounted indicator\n",
            "\n",
            "======================================================================\n",
            "STEP 9: EXPORTING CLEANED DATA\n",
            "======================================================================\n",
            "\n",
            "1. Saved cleaned dataset: cleaned_data/fashion_products_cleaned.csv\n",
            "   Shape: (11368, 14)\n",
            "2. Saved category-specific files to: cleaned_data/by_category\n",
            "3. Saved data summary: cleaned_data/data_summary.txt\n",
            "\n",
            "======================================================================\n",
            "PREPROCESSING COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "Final Dataset Statistics:\n",
            "  Total Products: 11368\n",
            "  Categories: 9\n",
            "  Brands: 2380\n",
            "  Price Range: 0.99 - 26000.00 AED\n",
            "  Average Rating: 4.21\n",
            "\n",
            "‚úì Cleaned data saved to: cleaned_data\n",
            "\n",
            "Next Step: Run image download script\n",
            "\n",
            "======================================================================\n",
            "Ready for image download and model training!\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Run the complete pipeline\n",
        "    csv_file = 'products.csv'  # Change to your file path\n",
        "\n",
        "    cleaned_df = run_preprocessing_pipeline(csv_file)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Ready for image download and model training!\")\n",
        "    print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt7NUJusXsLP",
        "outputId": "1791ff4c-a4c6-4895-b671-25447f258cf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset...\n",
            "‚úì Loaded 11,368 records\n",
            "‚úì Found 9 categories\n",
            "\n",
            "Generating visualizations...\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Class Balance Report:\n",
            "----------------------------------------------------------------------\n",
            "Categories with severe imbalance (>5x):\n",
            "  - Socks: 297 samples (8.10x)\n",
            "  - Underwear: 159 samples (15.13x)\n",
            "  - Sportswear: 101 samples (23.81x)\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "VISUALIZATION COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "All plots saved to: plots/\n",
            "  - category_distribution.png\n",
            "  - class_balance.png\n",
            "  - correlation_heatmap.png\n"
          ]
        }
      ],
      "source": [
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "# ==================== 1. CATEGORY DISTRIBUTION ====================\n",
        "\n",
        "def plot_category_distribution(df, save_path='plots'):\n",
        "    \"\"\"Visualize category distribution\"\"\"\n",
        "    Path(save_path).mkdir(exist_ok=True)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    # Bar chart\n",
        "    category_counts = df['product_category'].value_counts()\n",
        "    axes[0].barh(category_counts.index, category_counts.values, color='steelblue')\n",
        "    axes[0].set_xlabel('Number of Products', fontsize=12, fontweight='bold')\n",
        "    axes[0].set_title('Product Count by Category', fontsize=14, fontweight='bold')\n",
        "    axes[0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "    # Add count labels\n",
        "    for i, v in enumerate(category_counts.values):\n",
        "        axes[0].text(v + 50, i, str(v), va='center', fontweight='bold')\n",
        "\n",
        "    # Pie chart\n",
        "    colors = plt.cm.Set3(range(len(category_counts)))\n",
        "    wedges, texts, autotexts = axes[1].pie(\n",
        "        category_counts.values,\n",
        "        labels=category_counts.index,\n",
        "        autopct='%1.1f%%',\n",
        "        colors=colors,\n",
        "        startangle=90\n",
        "    )\n",
        "    axes[1].set_title('Category Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "    for autotext in autotexts:\n",
        "        autotext.set_color('white')\n",
        "        autotext.set_fontweight('bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_path}/category_distribution.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# ==================== 2. CLASS IMBALANCE ====================\n",
        "\n",
        "def plot_class_balance(df, save_path='plots'):\n",
        "    \"\"\"Check for class imbalance issues\"\"\"\n",
        "    Path(save_path).mkdir(exist_ok=True)\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "    category_counts = df['product_category'].value_counts()\n",
        "\n",
        "    # 1. Class balance visualization\n",
        "    axes[0].bar(range(len(category_counts)), category_counts.values, color='steelblue')\n",
        "    axes[0].set_xticks(range(len(category_counts)))\n",
        "    axes[0].set_xticklabels(category_counts.index, rotation=45, ha='right')\n",
        "    axes[0].set_ylabel('Number of Samples', fontsize=11, fontweight='bold')\n",
        "    axes[0].set_title('Class Balance Check', fontsize=13, fontweight='bold')\n",
        "    axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "    # Add threshold line for minimum recommended samples\n",
        "    min_recommended = 300\n",
        "    axes[0].axhline(y=min_recommended, color='r', linestyle='--',\n",
        "                    linewidth=2, label=f'Min Recommended: {min_recommended}')\n",
        "    axes[0].legend()\n",
        "\n",
        "    # Add value labels\n",
        "    for i, v in enumerate(category_counts.values):\n",
        "        axes[0].text(i, v + 50, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "    # 2. Imbalance ratio\n",
        "    max_samples = category_counts.max()\n",
        "    imbalance_ratios = max_samples / category_counts\n",
        "\n",
        "    colors = ['green' if ratio < 2 else 'orange' if ratio < 5 else 'red'\n",
        "              for ratio in imbalance_ratios]\n",
        "\n",
        "    axes[1].barh(category_counts.index, imbalance_ratios.values, color=colors)\n",
        "    axes[1].set_xlabel('Imbalance Ratio (Max/Current)', fontsize=11, fontweight='bold')\n",
        "    axes[1].set_title('Class Imbalance Analysis', fontsize=13, fontweight='bold')\n",
        "    axes[1].axvline(x=2, color='orange', linestyle='--', linewidth=1, alpha=0.5, label='Moderate (2x)')\n",
        "    axes[1].axvline(x=5, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Severe (5x)')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(axis='x', alpha=0.3)\n",
        "    axes[1].invert_yaxis()\n",
        "\n",
        "    # Add value labels\n",
        "    for i, v in enumerate(imbalance_ratios.values):\n",
        "        axes[1].text(v + 0.1, i, f'{v:.2f}x', va='center', fontweight='bold')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_path}/class_balance.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    # Print warnings\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Class Balance Report:\")\n",
        "    print(\"-\" * 70)\n",
        "    severe_imbalance = imbalance_ratios[imbalance_ratios > 5]\n",
        "    if len(severe_imbalance) > 0:\n",
        "        print(\"Categories with severe imbalance (>5x):\")\n",
        "        for cat in severe_imbalance.index:\n",
        "            print(f\"  - {cat}: {category_counts[cat]} samples ({imbalance_ratios[cat]:.2f}x)\")\n",
        "    else:\n",
        "        print(\"‚úì Class balance is acceptable (all classes within 5x of largest)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "# ==================== 3. CORRELATION HEATMAP ====================\n",
        "\n",
        "def plot_correlation_heatmap(df, save_path='plots'):\n",
        "    \"\"\"Plot correlation heatmap for numerical features\"\"\"\n",
        "    Path(save_path).mkdir(exist_ok=True)\n",
        "\n",
        "    # Select numerical columns\n",
        "    numeric_cols = ['price', 'rating', 'title_length', 'word_count']\n",
        "    numeric_cols = [col for col in numeric_cols if col in df.columns]\n",
        "\n",
        "    if len(numeric_cols) < 2:\n",
        "        print(\"Not enough numerical features for correlation analysis\")\n",
        "        return\n",
        "\n",
        "    corr_matrix = df[numeric_cols].corr()\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "                square=True, linewidths=1, fmt='.3f',\n",
        "                cbar_kws={\"shrink\": 0.8})\n",
        "    plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{save_path}/correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "# ==================== MAIN EXECUTION ====================\n",
        "\n",
        "def run_essential_visualizations(csv_path, plot_dir='plots'):\n",
        "    # Load cleaned data\n",
        "    print(\"Loading dataset...\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(f\"‚úì Loaded {len(df):,} records\")\n",
        "    print(f\"‚úì Found {df['product_category'].nunique()} categories\\n\")\n",
        "\n",
        "    # Generate visualizations\n",
        "    print(\"Generating visualizations...\\n\")\n",
        "\n",
        "    plot_category_distribution(df, plot_dir)\n",
        "    plot_class_balance(df, plot_dir)\n",
        "    plot_correlation_heatmap(df, plot_dir)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"VISUALIZATION COMPLETE!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nAll plots saved to: {plot_dir}/\")\n",
        "    print(\"  - category_distribution.png\")\n",
        "    print(\"  - class_balance.png\")\n",
        "    print(\"  - correlation_heatmap.png\")\n",
        "\n",
        "# ==================== USAGE ====================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run on cleaned data\n",
        "    csv_file = 'cleaned_data/fashion_products_cleaned.csv'\n",
        "\n",
        "    run_essential_visualizations(csv_file, plot_dir='plots')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==================== DOWNLOAD FUNCTION ====================\n",
        "\n",
        "def download_images(csv_path, output_dir='/content/drive/MyDrive/fashion_cnn/data/images'):\n",
        "    \"\"\"Download images from URLs and organize by category - SAVED TO GOOGLE DRIVE\"\"\"\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"DOWNLOADING IMAGES TO GOOGLE DRIVE\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Load cleaned data\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(f\"\\nTotal products to download: {len(df)}\")\n",
        "    print(f\"Categories: {df['product_category'].nunique()}\")\n",
        "\n",
        "    # Show category breakdown\n",
        "    print(\"\\nImages per category:\")\n",
        "    category_counts = df['product_category'].value_counts()\n",
        "    for cat, count in category_counts.items():\n",
        "        print(f\"  {cat:15s}: {count}\")\n",
        "\n",
        "    output_path = Path(output_dir)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(f\"\\nüìÅ Saving to Google Drive: {output_dir}\")\n",
        "    print(\"   (Images will persist even after Colab disconnects)\")\n",
        "\n",
        "    failed_downloads = []\n",
        "    success_count = 0\n",
        "    skipped_count = 0\n",
        "\n",
        "    print(f\"\\nStarting download...\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    total = len(df)\n",
        "\n",
        "    # Download with manual progress updates\n",
        "    for idx, row in df.iterrows():\n",
        "        category = row['product_category']\n",
        "        product_id = row['product_id']\n",
        "        image_url = row['image_url']\n",
        "\n",
        "        # Create category directory\n",
        "        category_dir = output_path / category\n",
        "        category_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        image_path = category_dir / f\"{product_id}.jpg\"\n",
        "\n",
        "        # Skip if already downloaded\n",
        "        if image_path.exists():\n",
        "            skipped_count += 1\n",
        "            # Print progress every 100 images\n",
        "            if (idx + 1) % 100 == 0:\n",
        "                print(f\"Progress: {idx + 1}/{total} ({(idx+1)/total*100:.1f}%) - Downloaded: {success_count}, Skipped: {skipped_count}, Failed: {len(failed_downloads)}\")\n",
        "            continue\n",
        "\n",
        "        # Download with retries\n",
        "        max_retries = 3\n",
        "        downloaded = False\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                # Set headers to mimic browser\n",
        "                headers = {\n",
        "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "                }\n",
        "\n",
        "                response = requests.get(image_url, timeout=15, headers=headers)\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    # Open and convert image\n",
        "                    img = Image.open(BytesIO(response.content))\n",
        "                    img = img.convert('RGB')\n",
        "\n",
        "                    # Save image to Google Drive\n",
        "                    img.save(image_path, 'JPEG', quality=95)\n",
        "                    success_count += 1\n",
        "                    downloaded = True\n",
        "                    break\n",
        "                else:\n",
        "                    if attempt == max_retries - 1:\n",
        "                        failed_downloads.append({\n",
        "                            'product_id': product_id,\n",
        "                            'category': category,\n",
        "                            'url': image_url,\n",
        "                            'error': f'HTTP {response.status_code}'\n",
        "                        })\n",
        "\n",
        "            except Exception as e:\n",
        "                if attempt == max_retries - 1:\n",
        "                    failed_downloads.append({\n",
        "                        'product_id': product_id,\n",
        "                        'category': category,\n",
        "                        'url': image_url,\n",
        "                        'error': str(e)[:100]\n",
        "                    })\n",
        "\n",
        "            # Wait before retry\n",
        "            if not downloaded and attempt < max_retries - 1:\n",
        "                time.sleep(1)\n",
        "\n",
        "        # Print progress every 100 images\n",
        "        if (idx + 1) % 100 == 0:\n",
        "            print(f\"Progress: {idx + 1}/{total} ({(idx+1)/total*100:.1f}%) - Downloaded: {success_count}, Skipped: {skipped_count}, Failed: {len(failed_downloads)}\")\n",
        "\n",
        "        # Rate limiting - pause every 100 images\n",
        "        if (idx + 1) % 100 == 0:\n",
        "            time.sleep(2)\n",
        "\n",
        "    # Final summary\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"DOWNLOAD COMPLETE!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\n‚úì Successfully downloaded: {success_count}\")\n",
        "    print(f\"‚äô Already existed (skipped): {skipped_count}\")\n",
        "    print(f\"‚úó Failed: {len(failed_downloads)}\")\n",
        "    print(f\"\\nTotal processed: {success_count + skipped_count + len(failed_downloads)}/{len(df)}\")\n",
        "\n",
        "    success_rate = ((success_count + skipped_count) / len(df)) * 100\n",
        "    print(f\"Success rate: {success_rate:.2f}%\")\n",
        "\n",
        "    # Save failed downloads to Google Drive\n",
        "    if len(failed_downloads) > 0:\n",
        "        failed_path = '/content/drive/MyDrive/fashion_cnn/failed_downloads.csv'\n",
        "        print(f\"\\n‚ö†Ô∏è  Some downloads failed. Details saved to Google Drive:\")\n",
        "        print(f\"   {failed_path}\")\n",
        "        failed_df = pd.DataFrame(failed_downloads)\n",
        "        failed_df.to_csv(failed_path, index=False)\n",
        "\n",
        "    # Verify directory structure\n",
        "    print(f\"\\n\" + \"=\" * 70)\n",
        "    print(\"VERIFICATION - FILES PER CATEGORY\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    for category in sorted(df['product_category'].unique()):\n",
        "        category_path = output_path / category\n",
        "        if category_path.exists():\n",
        "            file_count = len(list(category_path.glob('*.jpg')))\n",
        "            expected = len(df[df['product_category'] == category])\n",
        "            status = \"‚úì\" if file_count >= expected * 0.95 else \"‚ö†Ô∏è\"\n",
        "            print(f\"  {status} {category:15s}: {file_count:5d} / {expected:5d} images\")\n",
        "        else:\n",
        "            print(f\"  ‚úó {category:15s}: Directory not found!\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"üìÅ All images saved to Google Drive!\")\n",
        "    print(f\"   Location: {output_dir}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    return failed_downloads\n",
        "\n",
        "# ==================== RUN DOWNLOAD ====================\n",
        "\n",
        "print(\"Starting image download script...\")\n",
        "print(\"Images will be saved to Google Drive (permanent storage)\")\n",
        "print(\"This will take approximately 2-3 hours for 11,000+ images\\n\")\n",
        "\n",
        "# Update paths to use Google Drive\n",
        "csv_file = '/content/drive/MyDrive/fashion_products_cleaned.csv'\n",
        "output_folder = '/content/drive/MyDrive/fashion_cnn/data/images'\n",
        "\n",
        "# Run download\n",
        "failed = download_images(csv_file, output_dir=output_folder)\n",
        "\n",
        "print(\"\\n‚úÖ Download process complete!\")\n",
        "print(f\"‚úÖ {len(failed)} images failed to download\")\n",
        "print(\"\\nüìÅ Check your Google Drive: MyDrive/fashion_cnn/data/images/\")\n",
        "print(\"\\nNext step: Update training script to use Google Drive paths\")"
      ],
      "metadata": {
        "id": "-6t45RDhuoyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ==================== CONFIGURATION ====================\n",
        "\n",
        "class Config:\n",
        "    # Google Drive Paths\n",
        "    CSV_PATH = '/content/drive/MyDrive/fashion_products_cleaned.csv'\n",
        "    IMAGE_DIR = '/content/drive/MyDrive/fashion_cnn/data/images'\n",
        "    MODEL_SAVE_PATH = '/content/drive/MyDrive/fashion_cnn/models'\n",
        "    RESULTS_PATH = '/content/drive/MyDrive/fashion_cnn/results'\n",
        "\n",
        "    # Training parameters\n",
        "    BATCH_SIZE = 32\n",
        "    LEARNING_RATE = 0.001\n",
        "    NUM_EPOCHS = 25\n",
        "    EARLY_STOPPING_PATIENCE = 5\n",
        "\n",
        "    # Data split\n",
        "    TRAIN_SPLIT = 0.70\n",
        "    VAL_SPLIT = 0.15\n",
        "    TEST_SPLIT = 0.15\n",
        "\n",
        "    # Model\n",
        "    IMG_SIZE = 224\n",
        "    NUM_WORKERS = 2  # Reduced for Google Drive\n",
        "\n",
        "    # Device\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Random seed\n",
        "    RANDOM_SEED = 42\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(Config.RANDOM_SEED)\n",
        "np.random.seed(Config.RANDOM_SEED)\n",
        "\n",
        "# Create directories\n",
        "Path(Config.MODEL_SAVE_PATH).mkdir(parents=True, exist_ok=True)\n",
        "Path(Config.RESULTS_PATH).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ==================== DATASET CLASS ====================\n",
        "\n",
        "class FashionDataset(Dataset):\n",
        "    \"\"\"Custom Dataset for fashion images\"\"\"\n",
        "    def __init__(self, dataframe, img_dir, transform=None):\n",
        "        self.df = dataframe.reset_index(drop=True)\n",
        "        self.img_dir = Path(img_dir)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Create label mapping\n",
        "        self.categories = sorted(dataframe['product_category'].unique())\n",
        "        self.label_map = {cat: idx for idx, cat in enumerate(self.categories)}\n",
        "        self.idx_to_label = {idx: cat for cat, idx in self.label_map.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # Load image\n",
        "        img_path = self.img_dir / row['product_category'] / f\"{row['product_id']}.jpg\"\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            # Return a blank image if loading fails\n",
        "            image = Image.new('RGB', (Config.IMG_SIZE, Config.IMG_SIZE), color='white')\n",
        "\n",
        "        # Apply transforms\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Get label\n",
        "        label = self.label_map[row['product_category']]\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# ==================== DATA TRANSFORMS ====================\n",
        "\n",
        "def get_transforms(train=True):\n",
        "    \"\"\"Get data augmentation transforms\"\"\"\n",
        "    if train:\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.RandomCrop(Config.IMG_SIZE),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.RandomRotation(15),\n",
        "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    else:\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((Config.IMG_SIZE, Config.IMG_SIZE)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "# ==================== DATA PREPARATION ====================\n",
        "\n",
        "def prepare_data(csv_path):\n",
        "    \"\"\"Load and split data into train/val/test\"\"\"\n",
        "    print(\"=\" * 70)\n",
        "    print(\"PREPARING DATA\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(f\"\\nTotal products: {len(df)}\")\n",
        "    print(f\"Categories: {df['product_category'].nunique()}\")\n",
        "\n",
        "    # Category distribution\n",
        "    print(\"\\nCategory distribution:\")\n",
        "    print(df['product_category'].value_counts())\n",
        "\n",
        "    # Split: 70% train, 15% val, 15% test\n",
        "    train_df, temp_df = train_test_split(\n",
        "        df,\n",
        "        test_size=(Config.VAL_SPLIT + Config.TEST_SPLIT),\n",
        "        stratify=df['product_category'],\n",
        "        random_state=Config.RANDOM_SEED\n",
        "    )\n",
        "\n",
        "    val_df, test_df = train_test_split(\n",
        "        temp_df,\n",
        "        test_size=0.5,\n",
        "        stratify=temp_df['product_category'],\n",
        "        random_state=Config.RANDOM_SEED\n",
        "    )\n",
        "\n",
        "    print(f\"\\nData split:\")\n",
        "    print(f\"  Train: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%)\")\n",
        "    print(f\"  Val:   {len(val_df)} ({len(val_df)/len(df)*100:.1f}%)\")\n",
        "    print(f\"  Test:  {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)\")\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "def create_dataloaders(train_df, val_df, test_df):\n",
        "    \"\"\"Create PyTorch DataLoaders\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"CREATING DATALOADERS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = FashionDataset(train_df, Config.IMAGE_DIR, transform=get_transforms(train=True))\n",
        "    val_dataset = FashionDataset(val_df, Config.IMAGE_DIR, transform=get_transforms(train=False))\n",
        "    test_dataset = FashionDataset(test_df, Config.IMAGE_DIR, transform=get_transforms(train=False))\n",
        "\n",
        "    # Calculate class weights\n",
        "    class_counts = train_df['product_category'].value_counts().sort_index()\n",
        "    total_samples = len(train_df)\n",
        "    class_weights = torch.FloatTensor([total_samples / count for count in class_counts.values])\n",
        "    class_weights = class_weights.to(Config.DEVICE)\n",
        "\n",
        "    print(f\"\\nClass weights (for imbalance):\")\n",
        "    for cat, weight in zip(train_dataset.categories, class_weights):\n",
        "        print(f\"  {cat:15s}: {weight:.3f}\")\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    print(f\"\\nDataLoader info:\")\n",
        "    print(f\"  Batches per epoch (train): {len(train_loader)}\")\n",
        "    print(f\"  Batches per epoch (val):   {len(val_loader)}\")\n",
        "    print(f\"  Batches per epoch (test):  {len(test_loader)}\")\n",
        "\n",
        "    return train_loader, val_loader, test_loader, class_weights, train_dataset.categories\n",
        "\n",
        "# ==================== MODEL DEFINITION ====================\n",
        "\n",
        "class FashionClassifier(nn.Module):\n",
        "    \"\"\"ResNet50-based fashion classifier\"\"\"\n",
        "    def __init__(self, num_classes, pretrained=True):\n",
        "        super(FashionClassifier, self).__init__()\n",
        "\n",
        "        # Load pre-trained ResNet50\n",
        "        self.resnet = models.resnet50(pretrained=pretrained)\n",
        "\n",
        "        # Freeze early layers\n",
        "        for param in list(self.resnet.parameters())[:-30]:\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Get number of features\n",
        "        num_features = self.resnet.fc.in_features\n",
        "\n",
        "        # Replace final layer\n",
        "        self.resnet.fc = nn.Sequential(\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(num_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet(x)\n",
        "\n",
        "def create_model(num_classes):\n",
        "    \"\"\"Initialize model\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"CREATING MODEL\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    model = FashionClassifier(num_classes, pretrained=True)\n",
        "    model = model.to(Config.DEVICE)\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"\\nModel: ResNet50 with Transfer Learning\")\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"Frozen parameters: {total_params - trainable_params:,}\")\n",
        "    print(f\"Device: {Config.DEVICE}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# ==================== TRAINING FUNCTIONS ====================\n",
        "\n",
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "def validate(model, val_loader, criterion, device):\n",
        "    \"\"\"Validate the model\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(val_loader)\n",
        "    epoch_acc = 100. * correct / total\n",
        "\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "# ==================== MAIN TRAINING LOOP ====================\n",
        "\n",
        "def train_model(model, train_loader, val_loader, class_weights):\n",
        "    \"\"\"Complete training loop\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"TRAINING MODEL\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', patience=3, factor=0.5\n",
        "    )\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [], 'train_acc': [],\n",
        "        'val_loss': [], 'val_acc': []\n",
        "    }\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    print(f\"\\nStarting training for {Config.NUM_EPOCHS} epochs\")\n",
        "    print(f\"Batch size: {Config.BATCH_SIZE}\")\n",
        "    print(f\"Learning rate: {Config.LEARNING_RATE}\\n\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(Config.NUM_EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{Config.NUM_EPOCHS}\")\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        # Train\n",
        "        train_loss, train_acc = train_epoch(\n",
        "            model, train_loader, criterion, optimizer, Config.DEVICE\n",
        "        )\n",
        "\n",
        "        # Validate\n",
        "        val_loss, val_acc = validate(\n",
        "            model, val_loader, criterion, Config.DEVICE\n",
        "        )\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # Save history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Print results\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.2f}%\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            epochs_no_improve = 0\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_acc': val_acc,\n",
        "            }, f'{Config.MODEL_SAVE_PATH}/best_model.pth')\n",
        "            print(f\"‚úì Saved best model (Val Acc: {val_acc:.2f}%)\")\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if epochs_no_improve >= Config.EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"\\nEarly stopping after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nTraining complete! Time: {total_time/60:.2f} minutes\")\n",
        "    print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "    return history, best_val_acc\n",
        "\n",
        "# ==================== EVALUATION ====================\n",
        "\n",
        "def evaluate_model(model, test_loader, categories, device):\n",
        "    \"\"\"Evaluate on test set\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"EVALUATING ON TEST SET\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    test_acc = 100. * sum(np.array(all_preds) == np.array(all_labels)) / len(all_labels)\n",
        "    print(f\"\\nTest Accuracy: {test_acc:.2f}%\")\n",
        "\n",
        "    # Classification report\n",
        "    report = classification_report(all_labels, all_preds, target_names=categories, digits=4)\n",
        "    print(\"\\n\" + report)\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    return test_acc, cm, report\n",
        "\n",
        "# ==================== VISUALIZATION ====================\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Plot training history\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    axes[0].plot(history['train_loss'], label='Train', marker='o')\n",
        "    axes[0].plot(history['val_loss'], label='Val', marker='s')\n",
        "    axes[0].set_title('Loss', fontweight='bold')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(alpha=0.3)\n",
        "\n",
        "    axes[1].plot(history['train_acc'], label='Train', marker='o')\n",
        "    axes[1].plot(history['val_acc'], label='Val', marker='s')\n",
        "    axes[1].set_title('Accuracy', fontweight='bold')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].set_ylabel('%')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{Config.RESULTS_PATH}/training_history.png', dpi=300)\n",
        "    print(f\"‚úì Saved: {Config.RESULTS_PATH}/training_history.png\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_confusion_matrix(cm, categories):\n",
        "    \"\"\"Plot confusion matrix\"\"\"\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=categories, yticklabels=categories)\n",
        "    plt.title('Confusion Matrix', fontweight='bold')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{Config.RESULTS_PATH}/confusion_matrix.png', dpi=300)\n",
        "    print(f\"‚úì Saved: {Config.RESULTS_PATH}/confusion_matrix.png\")\n",
        "    plt.close()\n",
        "\n",
        "# ==================== MAIN ====================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main pipeline\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"FASHION CLASSIFICATION - TRAINING PIPELINE\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Device: {Config.DEVICE}\\n\")\n",
        "\n",
        "    # Check GPU\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"‚ö†Ô∏è  WARNING: GPU not available! Training will be VERY slow.\")\n",
        "        print(\"   Enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\\n\")\n",
        "\n",
        "    # Prepare data\n",
        "    train_df, val_df, test_df = prepare_data(Config.CSV_PATH)\n",
        "    train_loader, val_loader, test_loader, class_weights, categories = create_dataloaders(\n",
        "        train_df, val_df, test_df\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    model = create_model(len(categories))\n",
        "\n",
        "    # Train\n",
        "    history, best_val_acc = train_model(model, train_loader, val_loader, class_weights)\n",
        "\n",
        "    # Load best model\n",
        "    checkpoint = torch.load(f'{Config.MODEL_SAVE_PATH}/best_model.pth')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Evaluate\n",
        "    test_acc, cm, report = evaluate_model(model, test_loader, categories, Config.DEVICE)\n",
        "\n",
        "    # Plot results\n",
        "    plot_training_history(history)\n",
        "    plot_confusion_matrix(cm, categories)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"‚úÖ TRAINING COMPLETE!\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Best Val Acc: {best_val_acc:.2f}%\")\n",
        "    print(f\"Test Acc: {test_acc:.2f}%\")\n",
        "    print(f\"\\nüìÅ Model saved: {Config.MODEL_SAVE_PATH}/best_model.pth\")\n",
        "    print(f\"üìÅ Results saved: {Config.RESULTS_PATH}/\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "JVA6k14rurZr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}